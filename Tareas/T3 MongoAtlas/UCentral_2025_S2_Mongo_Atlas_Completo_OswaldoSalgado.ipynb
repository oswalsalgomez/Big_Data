{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1018WXkKAQmvRUwmxRhjpWMUTgzvXZm-0","timestamp":1758816924917}],"authorship_tag":"ABX9TyMLNb2k1Okfh3MchRpllK8P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Maestria en \"Analítica de Datos\"\n","---\n","\n","Nombre: Oswaldo Salgado Gómez\n","\n","Código: 79133277\n","\n","Fecha: Septiembre 25 de 2025\n","\n","Descripción: Este libro correspomde a la evaluación de la asignatura Big Data y tiene como objetivo dar respuesta al primer parcial Mongo Atlas para administrar bases de datos\n"],"metadata":{"id":"7Px7Q4GmRat4"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n7M7IKURbtGb","executionInfo":{"status":"ok","timestamp":1758835075569,"user_tz":300,"elapsed":777,"user":{"displayName":"oswaldo salgado gomez","userId":"00069353456977321735"}},"outputId":"b3ecb686-c285-4c75-8035-77b4cdb6f652"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"VkbWRbusvWr_"}},{"cell_type":"markdown","source":["# 0. Instanciar los Helpers (Python propios) para el proyecto final"],"metadata":{"id":"OVuSF---TWdE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLRZYiGObNi_"},"outputs":[],"source":["import os\n","import sys\n","import json\n","import pandas as pd\n","import time #captura el tiempo que se demoran las cosas\n","import subprocess\n","import importlib #peermitie importar la librería que hará parte del proyecto\n","from pymongo.errors import InvalidName # Importar InvalidName para manejar excepciones específicas\n","from tqdm import tqdm #muestra la barra de progreso\n","import requests\n","from bs4 import BeautifulSoup # Importar BeautifulSoup para parsear HTML (analizar y estructurar el código HTML de una página web para poder extraer información de ella.)\n","from urllib.parse import urljoin # Importar urljoin para manejar URLs relativas y lograr el pleno acceso al sitio donde están los datos\n","\n","ruta_proyecto=\"/content/drive/MyDrive/3_Semestre_3/Big_Data/ProyectoBigData/\" #no olvidar cerrar la ruta con \"/\" para indicar que es una carpeta\n","sys.path.append(ruta_proyecto+\"Helpers\")"]},{"cell_type":"code","source":["import functions\n","importlib.reload(functions) #por si no se carga functions\n","from functions import funciones #de fuctions se importan las funciones\n","# Crear una instancia de la clase\n","funciones= funciones()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DS_rsih6UCpO","executionInfo":{"status":"ok","timestamp":1758835076143,"user_tz":300,"elapsed":31,"user":{"displayName":"oswaldo salgado gomez","userId":"00069353456977321735"}},"outputId":"4b2d6829-e022-4d66-b7e6-3f1f5f8e3379"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clase funciones iniciada para uso inmediato\n"]}]},{"cell_type":"markdown","source":["#1. Instalar Librerias Especiales para conectarse a MongoAtlas"],"metadata":{"id":"vjM6hkmTVGHV"}},{"cell_type":"code","source":["!pip install pymongo\n","!pip install py2neo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LcBdGoBgVLKJ","executionInfo":{"status":"ok","timestamp":1758835088613,"user_tz":300,"elapsed":12469,"user":{"displayName":"oswaldo salgado gomez","userId":"00069353456977321735"}},"outputId":"bf6ae35e-112c-4f8d-819b-9d992661fda3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pymongo in /usr/local/lib/python3.12/dist-packages (4.15.1)\n","Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from pymongo) (2.8.0)\n","Requirement already satisfied: py2neo in /usr/local/lib/python3.12/dist-packages (2021.2.4)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from py2neo) (2025.8.3)\n","Requirement already satisfied: interchange~=2021.0.4 in /usr/local/lib/python3.12/dist-packages (from py2neo) (2021.0.4)\n","Requirement already satisfied: monotonic in /usr/local/lib/python3.12/dist-packages (from py2neo) (1.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from py2neo) (25.0)\n","Requirement already satisfied: pansi>=2020.7.3 in /usr/local/lib/python3.12/dist-packages (from py2neo) (2024.11.0)\n","Requirement already satisfied: pygments>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from py2neo) (2.19.2)\n","Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from py2neo) (1.17.0)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from py2neo) (2.5.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from interchange~=2021.0.4->py2neo) (2025.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from pansi>=2020.7.3->py2neo) (11.3.0)\n"]}]},{"cell_type":"markdown","source":["#2. Establecer la conexión a MongoAtlas"],"metadata":{"id":"UAgqUonoVVqr"}},{"cell_type":"code","source":["from pymongo import MongoClient\n","# Remplazar el <db_password> que se puso en MongoAtlas\n","uri = \"mongodb+srv://Oswaldo:Sanbonifacio13@cluster0.nb3onvq.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n","client = MongoClient(uri)\n","client.stats #muestra el status"],"metadata":{"id":"z1lotaNNXKVk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758835088658,"user_tz":300,"elapsed":42,"user":{"displayName":"oswaldo salgado gomez","userId":"00069353456977321735"}},"outputId":"0e863a9e-46d3-4366-da5c-1ca6691f1336"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Database(MongoClient(host=['ac-kvqaqou-shard-00-01.nb3onvq.mongodb.net:27017', 'ac-kvqaqou-shard-00-02.nb3onvq.mongodb.net:27017', 'ac-kvqaqou-shard-00-00.nb3onvq.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', appname='Cluster0', authsource='admin', replicaset='atlas-pm0j9t-shard-0', tls=True), 'stats')"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["#3. Descarga de los archivos"],"metadata":{"id":"sisioRRdttoD"}},{"cell_type":"markdown","source":["# Desde una URL"],"metadata":{"id":"LKe-MVV_5XdM"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2afb616b","executionInfo":{"status":"ok","timestamp":1758835088689,"user_tz":300,"elapsed":27,"user":{"displayName":"oswaldo salgado gomez","userId":"00069353456977321735"}},"outputId":"8784e671-f630-486e-d11b-9e23294ab739"},"source":["import requests\n","import os\n","\n","# Reemplaza con la URL de tu archivo zip\n","url_archivo_zip = \"URL_DEL_ARCHIVO_ZIP\"\n","# Define la ruta donde quieres guardar el archivo descargado\n","ruta_destino = \"/content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/nombre carpeta de descarga/\" # Asegúrate de que esta carpeta exista o créala\n","\n","if not os.path.exists(ruta_destino):\n","    os.makedirs(ruta_destino)\n","\n","try:\n","    response = requests.get(url_archivo_zip, stream=True)\n","    response.raise_for_status() # Lanza un error para códigos de estado HTTP incorrectos\n","\n","    nombre_archivo = url_archivo_zip.split('/')[-1]\n","    ruta_completa_destino = os.path.join(ruta_destino, nombre_archivo)\n","\n","    with open(ruta_completa_destino, 'wb') as f:\n","        for chunk in response.iter_content(chunk_size=8192):\n","            f.write(chunk)\n","\n","    print(f\"Archivo descargado exitosamente en: {ruta_completa_destino}\")\n","\n","except requests.exceptions.RequestException as e:\n","    print(f\"Error al descargar el archivo: {e}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Error al descargar el archivo: Invalid URL 'URL_DEL_ARCHIVO_ZIP': No scheme supplied. Perhaps you meant https://URL_DEL_ARCHIVO_ZIP?\n"]}]},{"cell_type":"markdown","source":["# Multiples archivos .zip en la URL"],"metadata":{"id":"NUGWNGctwT5_"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2c8845df","executionInfo":{"status":"ok","timestamp":1758835088700,"user_tz":300,"elapsed":10,"user":{"displayName":"oswaldo salgado gomez","userId":"00069353456977321735"}},"outputId":"8e148df8-bd94-4b9c-9e9a-417806ed30f1"},"source":["import requests\n","import os\n","from bs4 import BeautifulSoup # Importar BeautifulSoup para parsear HTML\n","from urllib.parse import urljoin # Importar urljoin para manejar URLs relativas\n","# Asegúrate de que la función 'funciones' esté importada y disponible\n","# from functions import funciones\n","\n","# Reemplaza con la URL de la página que lista los archivos zip\n","url_que_lista_zips = \"URL_DE_LA_PAGINA_CON_ZIPS\"\n","# Define la ruta donde quieres guardar los archivos descargados en Google Drive\n","ruta_destino_descarga = \"/content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/\" # Carpeta para los archivos zip descargados\n","\n","# Ruta donde se guardaran los archivos extraídos en Google Drive\n","ruta_destino_extraccion = \"/content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/JSON/\" # Carpeta para los archivos extraídos\n","\n","# Crear las carpetas de destino si no existen\n","if not os.path.exists(ruta_destino_descarga):\n","    os.makedirs(ruta_destino_descarga)\n","    print(f\"Carpeta de descarga creada: {ruta_destino_descarga}\")\n","else:\n","    print(f\"La carpeta de descarga ya existe: {ruta_destino_descarga}\")\n","\n","if not os.path.exists(ruta_destino_extraccion):\n","    os.makedirs(ruta_destino_extraccion)\n","    print(f\"Carpeta de extracción creada: {ruta_destino_extraccion}\")\n","else:\n","    print(f\"La carpeta de extracción ya existe: {ruta_destino_extraccion}\")\n","\n","\n","downloaded_zip_files = [] # Lista para almacenar las rutas de los archivos zip descargados\n","\n","try:\n","    response = requests.get(url_que_lista_zips)\n","    response.raise_for_status() # Lanza un error para códigos de estado HTTP incorrectos\n","    html_content = response.text\n","    print(\"Contenido HTML obtenido exitosamente.\")\n","\n","    # Parsear el HTML para encontrar enlaces a archivos zip\n","    soup = BeautifulSoup(html_content, 'html.parser')\n","    zip_links = []\n","    for link in soup.find_all('a', href=True):\n","        href = link['href']\n","        # Verificar si el enlace termina con .zip\n","        if href.endswith('.zip'):\n","            # Construir la URL completa si es relativa\n","            full_url = urljoin(url_que_lista_zips, href)\n","            zip_links.append(full_url)\n","\n","    print(f\"Enlaces a archivos zip encontrados: {zip_links}\")\n","\n","    # Descargar cada archivo zip encontrado\n","    if zip_links:\n","        print(f\"Iniciando descarga de {len(zip_links)} archivos zip...\")\n","        for zip_url in zip_links:\n","            try:\n","                print(f\"Descargando: {zip_url}\")\n","                file_name = zip_url.split('/')[-1]\n","                ruta_completa_destino_descarga = os.path.join(ruta_destino_descarga, file_name)\n","\n","                with requests.get(zip_url, stream=True) as r:\n","                    r.raise_for_status()\n","                    with open(ruta_completa_destino_descarga, 'wb') as f:\n","                        for chunk in r.iter_content(chunk_size=8192):\n","                            f.write(chunk)\n","                print(f\"Descarga completa de: {file_name}\")\n","                downloaded_zip_files.append(ruta_completa_destino_descarga) # Añadir a la lista de descargados\n","            except requests.exceptions.RequestException as e:\n","                print(f\"Error al descargar {zip_url}: {e}\")\n","    else:\n","        print(\"No se encontraron enlaces a archivos zip en la página.\")\n","\n","except requests.exceptions.RequestException as e:\n","    print(f\"Error al obtener el contenido de la URL principal: {e}\")\n","    html_content = None # Asegúrar que html_content sea None si hay un error\n","\n","# Descomprimir los archivos zip descargados\n","if downloaded_zip_files:\n","    print(f\"\\nIniciando descompresión de {len(downloaded_zip_files)} archivos zip descargados...\")\n","    for zip_file_path in downloaded_zip_files:\n","        print(f\"Descomprimiendo: {os.path.basename(zip_file_path)}\")\n","\n","        # Definir la ruta de destino para los archivos extraídos dentro de la carpeta de Drive que se indico. El nombre de la subcarpeta extraída será el nombre del archivo zip sin la extensión, NO ES NECESARIO CREAR LAS SUBCARPETAS\n","        extracted_folder_name = os.path.splitext(os.path.basename(zip_file_path))[0]\n","        ruta_extraccion_completa = os.path.join(ruta_destino_extraccion, extracted_folder_name)\n","\n","        # Descomprimir el zip mediante la función descomprimir_zip_local (asegurar que esa función maneje la creación de la carpeta de destino)\n","\n","        try:\n","            funciones.descomprimir_zip_local(zip_file_path, ruta_extraccion_completa)\n","            print(f\"Archivos de {os.path.basename(zip_file_path)} extraídos a: {ruta_extraccion_completa}\")\n","        except NameError:\n","            print(\"Error: La función 'funciones' o 'descomprimir_zip_local' no está definida.\")\n","            print(\"Verificar de haber ejecutado las celdas que importan y definen las funciones.\")\n","        except Exception as e:\n","            print(f\"Error al descomprimir {os.path.basename(zip_file_path)}: {e}\")\n","else:\n","    print(\"\\nNo hay archivos zip descargados para descomprimir.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["La carpeta de descarga ya existe: /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/\n","Carpeta de extracción creada: /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/JSON/\n","Error al obtener el contenido de la URL principal: Invalid URL 'URL_DE_LA_PAGINA_CON_ZIPS': No scheme supplied. Perhaps you meant https://URL_DE_LA_PAGINA_CON_ZIPS?\n","\n","No hay archivos zip descargados para descomprimir.\n"]}]},{"cell_type":"markdown","source":["# carga desde un github"],"metadata":{"id":"fNQryjuEvOmb"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0cbffa58","executionInfo":{"status":"ok","timestamp":1758835180145,"user_tz":300,"elapsed":91433,"user":{"displayName":"oswaldo salgado gomez","userId":"00069353456977321735"}},"outputId":"6a1b68cf-c5b7-4ec7-e9f0-577b3d717c25"},"source":["import os\n","\n","# Ingreando al GitHub donde se encuentra la información\n","github_username = 'oswalsalgomez' # Nombre del usuario de GitHub\n","github_repo_name = 'Big_Data' # Nombre de tu repositorio donde está la carpeta con la información\n","github_url = f'https://github.com/{github_username}/{github_repo_name}.git'\n","local_repo_path = f'/content/{github_repo_name}' # Ruta donde se clonará el repositorio\n","\n","# Ruta donde se guardan los archivos en Google Drive\n","ruta_destino_drive = \"/content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/JSON/\" # Se requiere ajustar el nombre de la carpeta\n","\n","# Crear la carpeta de destino en Drive si no existe\n","if not os.path.exists(ruta_destino_drive):\n","    os.makedirs(ruta_destino_drive)\n","    print(f\"Carpeta de destino en Drive creada: {ruta_destino_drive}\")\n","else:\n","    print(f\"La carpeta de destino en Drive ya existe: {ruta_destino_drive}\")\n","\n","# Clonar el repositorio si no existe\n","if not os.path.exists(local_repo_path):\n","    print(f\"Clonando repositorio: {github_url}\")\n","    !git clone {github_url}\n","else:\n","    print(f\"El repositorio ya existe en: {local_repo_path}\")\n","    # Opcional: puedes actualizar el repositorio si ya existe\n","    # %cd {local_repo_path}\n","    # !git pull\n","    # %cd /content/ # Vuelve al directorio original\n","\n","# Se requiere reemplazar la ruta donde están los archivos zip dentro del repositorio\n","carpeta_con_zips = os.path.join(local_repo_path, 'Parcial')\n","\n","# Verificar si la carpeta existe y lista los archivos zip\n","if os.path.exists(carpeta_con_zips):\n","    zip_files = [os.path.join(carpeta_con_zips, f) for f in os.listdir(carpeta_con_zips) if f.endswith('.zip')]\n","    print(f\"Archivos zip encontrados en {carpeta_con_zips}: {zip_files}\")\n","else:\n","    print(f\"La carpeta '{carpeta_con_zips}' no fue encontrada en el repositorio clonado.\")\n","    zip_files = [] # Inicializar zip_files como lista vacía si la carpeta no existe\n","\n","# Iterar en la lista 'zip_files' para procesar cada archivo zip\n","if zip_files:\n","    print(f\"Iniciando procesamiento de {len(zip_files)} archivos zip...\")\n","    for zip_file in zip_files:\n","        print(f\"Procesando archivo zip: {zip_file}\")\n","        # Definir la ruta de destino para los archivos extraídos dentro de la carpeta de Drive que se indico. El nombre de la subcarpeta extraída será el nombre del archivo zip sin la extensión, NO ES NECESARIO CREAR LAS SUBCARPETAS\n","        extracted_folder_name = os.path.splitext(os.path.basename(zip_file))[0]\n","        ruta_extraccion_drive = os.path.join(ruta_destino_drive, extracted_folder_name)\n","\n","        # Descomprimir el zip mediante la función descomprimir_zip_local (asegurar que esa función maneje la creación de la carpeta de destino)\n","        funciones.descomprimir_zip_local(zip_file, ruta_extraccion_drive)\n","        print(f\"Archivos de {os.path.basename(zip_file)} extraídos a: {ruta_extraccion_drive}\")\n","\n","else:\n","    print(\"No hay archivos zip para procesar en la carpeta especificada.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["La carpeta de destino en Drive ya existe: /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/JSON/\n","El repositorio ya existe en: /content/Big_Data\n","Archivos zip encontrados en /content/Big_Data/Parcial: ['/content/Big_Data/Parcial/MicroMercado_ElMio.zip', '/content/Big_Data/Parcial/Despensa_Natura.zip', '/content/Big_Data/Parcial/FrutiBarato.zip', '/content/Big_Data/Parcial/Supermercado_Central.zip']\n","Iniciando procesamiento de 4 archivos zip...\n","Procesando archivo zip: /content/Big_Data/Parcial/MicroMercado_ElMio.zip\n","Carpeta creada: /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/JSON/MicroMercado_ElMio\n"]},{"output_type":"stream","name":"stderr","text":["Descomprimiendo: 100%|██████████| 2514/2514 [00:33<00:00, 75.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Archivos de MicroMercado_ElMio.zip extraídos a: /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/JSON/MicroMercado_ElMio\n","Procesando archivo zip: /content/Big_Data/Parcial/Despensa_Natura.zip\n","Carpeta creada: /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/JSON/Despensa_Natura\n"]},{"output_type":"stream","name":"stderr","text":["Descomprimiendo: 100%|██████████| 1144/1144 [00:13<00:00, 85.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Archivos de Despensa_Natura.zip extraídos a: /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/JSON/Despensa_Natura\n","Procesando archivo zip: /content/Big_Data/Parcial/FrutiBarato.zip\n","Carpeta creada: /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/JSON/FrutiBarato\n"]},{"output_type":"stream","name":"stderr","text":["Descomprimiendo: 100%|██████████| 1858/1858 [00:22<00:00, 83.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Archivos de FrutiBarato.zip extraídos a: /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/JSON/FrutiBarato\n","Procesando archivo zip: /content/Big_Data/Parcial/Supermercado_Central.zip\n","Carpeta creada: /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/JSON/Supermercado_Central\n"]},{"output_type":"stream","name":"stderr","text":["Descomprimiendo: 100%|██████████| 1879/1879 [00:22<00:00, 83.68it/s]"]},{"output_type":"stream","name":"stdout","text":["Archivos de Supermercado_Central.zip extraídos a: /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/JSON/Supermercado_Central\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["#4. Crear la Base de Datos"],"metadata":{"id":"Uq7gkjkL5O1-"}},{"cell_type":"code","source":["db_name=\"parcial\" #nombre de la base de datos\n","db=client[db_name] #crear base datos en MongoAtlas\n","print(f\"Base de datos {db_name} creada exitosamente. Las colecciones se crearán dinámicamente.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oc8dqSnl3VxT","executionInfo":{"status":"ok","timestamp":1758835180167,"user_tz":300,"elapsed":6,"user":{"displayName":"oswaldo salgado gomez","userId":"00069353456977321735"}},"outputId":"a0751c8b-0492-46b3-9057-02995df2f71e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Base de datos parcial creada exitosamente. Las colecciones se crearán dinámicamente.\n"]}]},{"cell_type":"markdown","source":["#5. Creación de las colecciones para cada uno de las subcarpetas y cargando a MongoAtlas"],"metadata":{"id":"Zmtuuk7o7DzU"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a2f1f52f","executionInfo":{"status":"ok","timestamp":1758835330662,"user_tz":300,"elapsed":150482,"user":{"displayName":"oswaldo salgado gomez","userId":"00069353456977321735"}},"outputId":"9ac3de79-ded6-4b64-c00a-0ffaf0910e38"},"source":["import json\n","import os\n","from tqdm import tqdm\n","from pymongo.errors import InvalidName\n","\n","# Ruta donde se encuentran los archivos JSON descomprimidos en Google Drive\n","# Esta es la misma ruta que definimos en la celda anterior para guardar los archivos extraídos\n","ruta_json = \"/content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/JSON/\"\n","\n","if 'db' not in globals() or db is None:\n","    print(\"Error: No se encontró una conexión activa a la base de datos de MongoDB ('db').\")\n","    print(\"Por favor, asegúrate de que la conexión a MongoAtlas esté establecida en las celdas anteriores.\")\n","else:\n","    print(f\"Conexión a la base de datos '{db.name}' de MongoDB encontrada.\")\n","    print(f\"Iniciando carga de archivos JSON a colecciones de MongoDB desde: {ruta_json}\")\n","\n","    # Recorrer las subcarpetas dentro de la ruta donde se descomprimieron los zips\n","    for root, dirs, files in os.walk(ruta_json):\n","        # Excluir la carpeta raíz si no contiene archivos JSON directamente que quieras cargar\n","        # También excluir directorios que comienzan con punto (como .ipynb_checkpoints)\n","        dirs[:] = [d for d in dirs if not d.startswith('.')] # Modificar dirs in-place to skip dotted directories\n","        if root == ruta_json and not any(f.endswith('.json') for f in files):\n","             continue\n","\n","        subcarpeta = os.path.basename(root)\n","\n","        # Saltar si el nombre de la subcarpeta es inválido para una colección de MongoDB\n","        if subcarpeta.startswith('.') or subcarpeta.endswith('.'):\n","             print(f\"Saltando subcarpeta con nombre inválido para colección de MongoDB: {subcarpeta}\")\n","             continue\n","\n","        print(f\"\\nProcesando subcarpeta: {subcarpeta}\")\n","\n","        # Crear/seleccionar la colección en MongoDB con el nombre de la subcarpeta\n","        try:\n","            collection = db[subcarpeta]\n","            print(f\"Seleccionada colección: {collection.name}\")\n","        except InvalidName as e:\n","            print(f\"Error: El nombre de la subcarpeta '{subcarpeta}' es inválido para una colección de MongoDB. Saltando. Error: {e}\")\n","            continue\n","\n","\n","        file_list = [os.path.join(root, f) for f in files if f.endswith('.json')]\n","        print(f\"Cantidad de archivos JSON a cargar en '{subcarpeta}': {len(file_list)}\")\n","\n","        if not file_list:\n","            print(f\"No se encontraron archivos JSON en '{subcarpeta}'. Saltando...\")\n","            continue\n","\n","        # Crear lista para almacenar los datos de los archivos JSON para inserción masiva\n","        data_list = []\n","\n","        with tqdm(total=len(file_list), desc=f\"Cargando archivos JSON a '{subcarpeta}' \") as pbar:\n","            for file_path in file_list:\n","                with open(file_path, 'r', encoding='utf-8') as file: # Especificar encoding si se requiere\n","                    try:\n","                        data = json.load(file)\n","                        # Si el archivo JSON contiene una lista de objetos, añadir cada objeto a data_list\n","                        if isinstance(data, list):\n","                            data_list.extend(data)\n","                        # Si el archivo JSON contiene un solo objeto, añadirlo a data_list\n","                        elif isinstance(data, dict):\n","                            data_list.append(data)\n","                        else:\n","                             print(f\"Advertencia: Archivo JSON '{os.path.basename(file_path)}' no contiene un objeto o lista de objetos válido. Saltando.\")\n","\n","                    except json.JSONDecodeError as e:\n","                        print(f\"Error al decodificar el archivo JSON {os.path.basename(file_path)}: {e}\")\n","                    except Exception as e:\n","                        print(f\"Error inesperado al procesar el archivo {os.path.basename(file_path)}: {e}\")\n","\n","                pbar.update(1)\n","\n","        # Insertar los datos recopilados de todos los archivos JSON de la subcarpeta en la colección\n","        if data_list:\n","            try:\n","                print(f\"Insertando {len(data_list)} documentos en la colección '{collection.name}'...\")\n","                result = collection.insert_many(data_list)\n","                print(f\"Inserción masiva completa. Documentos insertados: {len(result.inserted_ids)}\")\n","            except Exception as e:\n","                print(f\"Error al insertar documentos en la colección '{collection.name}': {e}\")\n","        else:\n","            print(f\"No se recopilaron datos válidos de los archivos JSON en '{subcarpeta}' para insertar.\")\n","\n","    print(\"\\nProceso de carga de archivos JSON a MongoDB finalizado.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Conexión a la base de datos 'parcial' de MongoDB encontrada.\n","Iniciando carga de archivos JSON a colecciones de MongoDB desde: /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/JSON/\n","\n","Procesando subcarpeta: MicroMercado_ElMio\n","Seleccionada colección: MicroMercado_ElMio\n","Cantidad de archivos JSON a cargar en 'MicroMercado_ElMio': 0\n","No se encontraron archivos JSON en 'MicroMercado_ElMio'. Saltando...\n","\n","Procesando subcarpeta: MicroMercado_ElMio\n","Seleccionada colección: MicroMercado_ElMio\n","Cantidad de archivos JSON a cargar en 'MicroMercado_ElMio': 2514\n"]},{"output_type":"stream","name":"stderr","text":["Cargando archivos JSON a 'MicroMercado_ElMio' : 100%|██████████| 2514/2514 [00:54<00:00, 46.18it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Insertando 2514 documentos en la colección 'MicroMercado_ElMio'...\n","Inserción masiva completa. Documentos insertados: 2514\n","\n","Procesando subcarpeta: Despensa_Natura\n","Seleccionada colección: Despensa_Natura\n","Cantidad de archivos JSON a cargar en 'Despensa_Natura': 0\n","No se encontraron archivos JSON en 'Despensa_Natura'. Saltando...\n","\n","Procesando subcarpeta: Despensa_Natura\n","Seleccionada colección: Despensa_Natura\n","Cantidad de archivos JSON a cargar en 'Despensa_Natura': 1144\n"]},{"output_type":"stream","name":"stderr","text":["Cargando archivos JSON a 'Despensa_Natura' : 100%|██████████| 1144/1144 [00:13<00:00, 82.14it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Insertando 1144 documentos en la colección 'Despensa_Natura'...\n","Inserción masiva completa. Documentos insertados: 1144\n","\n","Procesando subcarpeta: FrutiBarato\n","Seleccionada colección: FrutiBarato\n","Cantidad de archivos JSON a cargar en 'FrutiBarato': 0\n","No se encontraron archivos JSON en 'FrutiBarato'. Saltando...\n","\n","Procesando subcarpeta: FrutiBarato\n","Seleccionada colección: FrutiBarato\n","Cantidad de archivos JSON a cargar en 'FrutiBarato': 1858\n"]},{"output_type":"stream","name":"stderr","text":["Cargando archivos JSON a 'FrutiBarato' : 100%|██████████| 1858/1858 [00:29<00:00, 63.30it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Insertando 1858 documentos en la colección 'FrutiBarato'...\n","Inserción masiva completa. Documentos insertados: 1858\n","\n","Procesando subcarpeta: Supermercado_Central\n","Seleccionada colección: Supermercado_Central\n","Cantidad de archivos JSON a cargar en 'Supermercado_Central': 0\n","No se encontraron archivos JSON en 'Supermercado_Central'. Saltando...\n","\n","Procesando subcarpeta: Supermercado_Central\n","Seleccionada colección: Supermercado_Central\n","Cantidad de archivos JSON a cargar en 'Supermercado_Central': 1879\n"]},{"output_type":"stream","name":"stderr","text":["Cargando archivos JSON a 'Supermercado_Central' : 100%|██████████| 1879/1879 [00:24<00:00, 75.58it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Insertando 1879 documentos en la colección 'Supermercado_Central'...\n","Inserción masiva completa. Documentos insertados: 1879\n","\n","Proceso de carga de archivos JSON a MongoDB finalizado.\n"]}]},{"cell_type":"markdown","source":["#3. Borrar información en el Drive desde Colab"],"metadata":{"id":"pUqIigieaKVQ"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"366a0324","executionInfo":{"status":"ok","timestamp":1758835352018,"user_tz":300,"elapsed":21353,"user":{"displayName":"oswaldo salgado gomez","userId":"00069353456977321735"}},"outputId":"f3f9f3d4-d074-4a34-b553-0dc815becab3"},"source":["#como borrar información en el drive desde Colab. Se puede borrar desde el drive directamente\n","\n","!rm -r /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/Despensa_Natura\n","\n","!rm -r /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/JSON\n","\n","!rm -r /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/FrutiBarato\n","\n","!rm -r /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/MicroMercado_ElMio\n","\n","!rm -r /content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/Supermercado_Central\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove '/content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/Despensa_Natura': No such file or directory\n","rm: cannot remove '/content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/FrutiBarato': No such file or directory\n","rm: cannot remove '/content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/MicroMercado_ElMio': No such file or directory\n","rm: cannot remove '/content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion/Supermercado_Central': No such file or directory\n"]}]},{"cell_type":"markdown","source":["7. Carga de archivos csv"],"metadata":{"id":"oAyTrWl9yXxh"}},{"cell_type":"code","metadata":{"id":"3526524f"},"source":["import os\n","import pandas as pd\n","from tqdm import tqdm # Import tqdm for progress bar\n","from pymongo.errors import InvalidName # Importar InvalidName para manejar excepciones específicas\n","\n","# Definir la ruta donde se encuentran las carpetas con archivos CSV descomprimidos.\n","ruta_csv_descomprimidos = \"/content/drive/MyDrive/3_Semestre_3/Big_Data/DataSets_TMP/Facturacion_CSV/\"\n","\n","if 'db' not in globals() or db is None:\n","    print(\"Error: No se encontró una conexión activa a la base de datos de MongoDB ('db').\")\n","    print(\"Por favor, asegúrate de que la conexión a MongoAtlas esté establecida en las celdas anteriores.\")\n","else:\n","    print(f\"Conexión a la base de datos '{db.name}' de MongoDB encontrada.\")\n","    print(f\"Iniciando carga de archivos CSV a colecciones de MongoDB desde subcarpetas en: {ruta_csv_descomprimidos}\")\n","\n","    # Obtener solo los nombres de los directorios inmediatos dentro de la ruta principal\n","    try:\n","        subcarpeta_names = [d for d in os.listdir(ruta_csv_descomprimidos) if os.path.isdir(os.path.join(ruta_csv_descomprimidos, d)) and not d.startswith('.')]\n","    except FileNotFoundError:\n","        print(f\"Error: La ruta principal '{ruta_csv_descomprimidos}' no fue encontrada.\")\n","        subcarpeta_names = []\n","\n","    if not subcarpeta_names:\n","        print(f\"No se encontraron subcarpetas (que no empiecen con punto) en '{ruta_csv_descomprimidos}'. No se cargarán colecciones.\")\n","    else:\n","        print(f\"Se encontraron {len(subcarpeta_names)} subcarpetas para procesar: {subcarpeta_names}\")\n","\n","        # Recorrer las subcarpetas encontradas\n","        for subcarpeta in subcarpeta_names:\n","            subcarpeta_path = os.path.join(ruta_csv_descomprimidos, subcarpeta)\n","\n","            # Saltar si el nombre de la subcarpeta es inválido para una colección de MongoDB\n","            if subcarpeta.endswith('.'):\n","                 print(f\"Saltando subcarpeta con nombre inválido para colección de MongoDB: {subcarpeta}\")\n","                 continue\n","\n","            print(f\"\\nProcesando subcarpeta: {subcarpeta}\")\n","\n","            # Crear o seleccionar la colección en MongoDB con el nombre de la subcarpeta\n","            try:\n","                collection = db[subcarpeta]\n","                print(f\"Seleccionada colección: {collection.name}\")\n","            except InvalidName as e:\n","                print(f\"Error: El nombre de la subcarpeta '{subcarpeta}' es inválido para una colección de MongoDB. Saltando. Error: {e}\")\n","                continue\n","\n","            # Encontrar todos los archivos CSV dentro de esta subcarpeta específica\n","            file_list = [os.path.join(subcarpeta_path, f) for f in os.listdir(subcarpeta_path) if f.endswith('.csv')]\n","            print(f\"Cantidad de archivos CSV a cargar en '{subcarpeta}': {len(file_list)}\")\n","\n","            if not file_list:\n","                print(f\"No se encontraron archivos CSV en '{subcarpeta_path}'. Saltando la carga de esta colección...\")\n","                continue\n","\n","            # Lista para almacenar los datos de los archivos CSV para inserción masiva\n","            data_list = []\n","\n","            with tqdm(total=len(file_list), desc=f\"Cargando archivos CSV a '{subcarpeta}' \") as pbar:\n","                for file_path in file_list:\n","                    try:\n","                        # Leer el archivo CSV usando pandas\n","                        df = pd.read_csv(file_path)\n","\n","                        # Convertir el DataFrame a una lista de diccionarios (orient='records' es común para MongoDB)\n","                        # Manejar posibles valores NaN si es necesario (por ejemplo, .dropna() o .fillna())\n","                        data = df.to_dict(orient='records')\n","\n","                        # Añadir los datos a la lista general para inserción masiva\n","                        data_list.extend(data)\n","\n","                    except FileNotFoundError:\n","                        print(f\"Error: Archivo CSV no encontrado en '{file_path}'. Saltando.\")\n","                    except pd.errors.EmptyDataError:\n","                        print(f\"Advertencia: El archivo CSV '{os.path.basename(file_path)}' está vacío. Saltando.\")\n","                    except Exception as e:\n","                        print(f\"Error inesperado al procesar el archivo CSV {os.path.basename(file_path)} en '{subcarpeta}': {e}\")\n","\n","                    pbar.update(1)\n","\n","            # Insertar los datos recopilados de todos los archivos CSV de la subcarpeta en la colección\n","            if data_list:\n","                try:\n","                    print(f\"Insertando {len(data_list)} documentos en la colección '{collection.name}'...\")\n","                    # MongoDB insert_many requiere una lista de diccionarios\n","                    result = collection.insert_many(data_list)\n","                    print(f\"Inserción masiva completa. Documentos insertados: {len(result.inserted_ids)}\")\n","                except Exception as e:\n","                    print(f\"Error al insertar documentos en la colección '{collection.name}': {e}\")\n","            else:\n","                print(f\"No se recopilaron datos válidos de los archivos CSV en '{subcarpeta}' para insertar.\")\n","\n","        print(\"\\nProceso de carga de archivos CSV a MongoDB finalizado.\")"],"execution_count":null,"outputs":[]}]}